# MLflow Integration Summary (Ticket 5.2)

## Overview
Successfully integrated MLflow for experiment tracking into the causal discovery experimental framework, replacing the manual CSV logging system with a robust and queryable backend for experiment analysis.

## Implementation Details

### 1. Environment Setup
- Added `mlflow` to `environment.yml` dependencies
- Added `mlruns/` directory to `.gitignore` to exclude MLflow tracking files from version control
- MLflow 3.1.4 successfully installed

### 2. Core MLflow Utilities (`utils/mlflow_utils.py`)
Created utility functions for MLflow integration:

#### `flatten_config(config, parent_key='', sep='.')`
- Flattens nested configuration dictionaries for MLflow parameter logging
- Converts lists to comma-separated strings
- Handles MLflow's parameter value length limits (250 characters)

#### `log_config_as_params(config, learner_name=None)`
- Logs flattened configuration as MLflow parameters
- Optionally includes learner name in parameter names
- Truncates long parameter values to meet MLflow limits

#### `log_artifacts_selectively(results_dir, learner_name=None)`
- Logs specific artifacts while excluding plots to keep tracking lightweight
- Logs general experiment artifacts: `config.yaml`, `ground_truth.yaml`, `ground_truth_theta.pkl`, `metrics.yaml`, `intervention_details.yaml`
- Logs learner-specific artifacts: `particles.npz`, `thetas.pkl`

### 3. Updated `run_experiment.py`
Modified the main experiment script with the following changes:

#### MLflow Setup
- Import MLflow and utility functions
- Set experiment name to "dibs_vs_ensembles" using `mlflow.set_experiment()`

#### Experiment Tracking
- Wrapped main experiment logic within `mlflow.start_run()` context
- Log global configuration parameters at experiment start
- For each learner:
  - Log learner-specific configuration parameters
  - Log evaluation metrics with learner name prefix (e.g., `"SVGD.eshd"`, `"Ensemble.auroc"`)
  - Maintain backward compatibility with CSV logging

#### Artifact Logging
- Log key data artifacts excluding plots:
  - Ground truth graph and parameters
  - Learned particles and model parameters
  - Configuration and metrics files
- Exclude visualization plots to keep MLflow runs lightweight

### 4. Key Features

#### Comprehensive Parameter Logging
All hyperparameters are logged as MLflow parameters:
- Experiment configuration (random seed, data generation settings)
- Model architecture (hidden layers, noise parameters)
- Training parameters (number of steps)
- Learner-specific settings (particles, ensemble runs)
- Evaluation settings (metrics, intervention values)

#### Structured Metrics Logging
Evaluation metrics are logged with learner name prefixes:
- `SVGD.eshd`, `SVGD.auroc`, `SVGD.negll_obs`, `SVGD.negll_intrv`
- `Ensemble.eshd`, `Ensemble.auroc`, `Ensemble.negll_obs`, `Ensemble.negll_intrv`

#### Selective Artifact Storage
- **Included**: Configuration files, ground truth data, learned parameters, metrics
- **Excluded**: Plot images (to keep tracking lightweight as specified)
- Artifacts are organized by learner name when applicable

#### Backward Compatibility
- CSV logging remains functional for transition period
- Can be easily disabled by commenting out the `learner.save_to_csv()` call

### 5. Usage

The MLflow integration is now active by default. To run an experiment with MLflow tracking:

```bash
python run_experiment.py --config configs/dibs_vs_ensemble.yaml
```

MLflow data will be stored locally in the `mlruns/` directory. The run ID is displayed at the end of each experiment.

### 6. Benefits

1. **Queryable Backend**: Easily search and filter experiments by parameters and metrics
2. **Structured Storage**: Organized parameter and metric logging with proper data types
3. **Artifact Management**: Systematic storage of key experimental artifacts
4. **Scalability**: Handles large numbers of hyperparameter sweep runs efficiently
5. **Reproducibility**: Complete parameter and artifact logging for experiment reproduction

### 7. Acceptance Criteria Fulfilled

✅ **MLflow Integration**: `run_experiment.py` updated with MLflow logging  
✅ **Run Creation**: MLflow run created before experiment starts (`mlflow.start_run()`)  
✅ **Parameter Logging**: All hyperparameters logged using `mlflow.log_params()`  
✅ **Metrics Logging**: Final evaluation scores logged using `mlflow.log_metrics()`  
✅ **Artifact Logging**: Key data files saved using `mlflow.log_artifact()`  
✅ **Lightweight Tracking**: Plot images excluded from MLflow artifacts  
✅ **Environment Setup**: MLflow added to dependencies, `mlruns/` added to `.gitignore`

The implementation provides a robust foundation for managing the large number of runs generated by hyperparameter sweeps while maintaining the ability to regenerate plots from stored data artifacts when needed.